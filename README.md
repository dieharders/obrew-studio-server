# üç∫ Obrew Studio: Server - Your Personal Ai Engine

![banner](assets/images/banner.png)

[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)
<br>
![Python](https://img.shields.io/badge/-Python-000?&logo=Python)
![Javascript](https://img.shields.io/badge/-JavaScript-000?&logo=JavaScript)
![FastAPI](https://img.shields.io/badge/-FastAPI-000?&logo=fastapi)

## Table of Contents

- [Supported Models](https://huggingface.co/models?library=gguf&sort=trending)
- [Features](#app-features-roadmap)
- [How to Use](assets/how-to-use.md)
- [Getting Started](assets/getting-started.md)
- [API Documentation](assets/api-docs.md)
- [Build Steps](assets/build-steps.md)
- [Bundling for Release](assets/bundling-for-release.md)
- [Deploy](assets/deploy.md)

## Introduction

The goal of this project is to be an all-in-one solution for running local Ai that is easy to install, setup and use. It handles all basic building blocks of Ai: inference, memory retrieval (RAG) and storage (vector DB), model file management, and agent/workflow building.

## Description

Obrew Studio is a native app with a GUI that can be configured to allow access from other apps you write or third party services, making it an ideal engine for Ai workloads built on your own tech stack.

## How It Works

![obrew](assets/images/doc-poster.png)

This backend runs a web server that acts as the main gateway to the suite of tools. A WebUI is provided called [Obrew Studio: WebUI](https://studio.openbrewai.com/) to access this server. You can also run in headless mode to access it programmatically via the API.

To use Ai locally with your private data for free:

- Launch the desktop app and use the GUI to start building
- or navigate your browser to any web app that supports the api
- or connect with a service provider or custom stack that supports the api

## App Features Roadmap

- ‚úÖ Run locally
- ‚úÖ Desktop installers
- ‚úÖ Save chat history
- ‚úÖ CPU & GPU support
- ‚úÖ Windows OS installer
- ‚ùå MacOS/Linux installer
- ‚ùå Docker config for cloud/server deployment
- ‚ùå Production ready: This project is currently under active development

## Ai Features Roadmap

- ‚úÖ Inference: Run open-source LLM models locally
- ‚úÖ Embeddings: Create vector embeddings from a file/website/media to augment memory
- ‚úÖ Knowledge Base: Search a vector database with Llama Index to retrieve information
- ‚úÖ Agents: Customized LLM, can choose or specify tool use
- ‚úÖ Tool Use: Choose from pre-made or write your own
- ‚ùå Workflows: Composable automation of tasks, teams of agents, parallel processing, conditional routing
- ‚ùå Monitors: Source citations, observability, logging, time-travel, transparency
- ‚ùå Support multi-modal: vision, text, audio, 3d, and beyond
- ‚ùå Support multi-device memory sharing (i.e. cluster of macs running single large model)
- ‚ùå Support voice-to-text and text-to-speech
- ‚ùå Auto Agents: Self-prompting, autonomous agent given tools and access to a sandboxed OS env

## Supported Model Providers

This is a local first project. The ultimate goal is to support many providers via one API.

- ‚úÖ [Open-Source](https://huggingface.co)
- ‚ùå [Google Gemini](https://gemini.google.com)
- ‚ùå [OpenAI](https://openai.com/chatgpt)
- ‚ùå [Anthropic](https://www.anthropic.com)
- ‚ùå [Mistral AI](https://mistral.ai)
- ‚ùå [Groq](https://groq.com)

## Learn More

- Backend: [FastAPI](https://fastapi.tiangolo.com/) - learn about FastAPI features and API.
- Inference: [llama-cpp](https://github.com/ggerganov/llama.cpp) and [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) for LLM inference.
- Memory: [Llama-Index](https://github.com/run-llama/llama_index) for data retrieval and [ChromaDB](https://github.com/chroma-core/chroma) for vector database.
- WebUI: Vanilla HTML and [Next.js](https://nextjs.org/) for front-end UI and [Pywebview](https://github.com/r0x0r/pywebview) for rendering the webview.
