name: Build llama.cpp

on:
  workflow_dispatch:
    inputs:
      release_tag:
        description: 'Specific release tag or commit to build (leave empty to use package.json)'
        required: false
        type: string
      retention_days:
        description: 'Number of days to retain artifacts'
        required: false
        type: number
        default: 7

jobs:
  build-macos-metal:
    name: macOS (Metal)
    runs-on: macos-latest

    steps:
      - name: Checkout project repository
        uses: actions/checkout@v4

      - name: Read llama.cpp tag from package.json
        id: llama_tag
        run: |
          LLAMA_TAG=$(grep '"llamacpp_tag"' package.json | cut -d'"' -f4)
          echo "tag=$LLAMA_TAG" >> $GITHUB_OUTPUT
          echo "Building llama.cpp at tag: $LLAMA_TAG"

      - name: Clone llama.cpp repository
        uses: actions/checkout@v4
        with:
          repository: ggml-org/llama.cpp
          ref: ${{ github.event.inputs.release_tag || steps.llama_tag.outputs.tag }}
          submodules: recursive
          path: llama-cpp-build

      - name: Install dependencies
        run: |
          brew install cmake
          # Verify Xcode command line tools for Metal compiler
          xcode-select --print-path

      - name: Configure with Metal and Accelerate (Universal Binary)
        working-directory: llama-cpp-build
        run: |
          cmake -B build \
            -DCMAKE_BUILD_TYPE=Release \
            -DCMAKE_OSX_ARCHITECTURES="arm64;x86_64" \
            -DGGML_METAL=ON \
            -DLLAMA_CURL=OFF \
            -DLLAMA_BUILD_TESTS=OFF \
            -DLLAMA_BUILD_EXAMPLES=ON \
            -DLLAMA_NATIVE=OFF

      - name: Build
        working-directory: llama-cpp-build
        run: |
          cmake --build build --config Release -j$(sysctl -n hw.ncpu)

      - name: Test Metal support
        working-directory: llama-cpp-build
        run: |
          # Verify binaries are built
          if [ -f "build/bin/llama-cli" ]; then
            file build/bin/llama-cli
            echo "Binary built successfully"
          fi

      - name: Fix dylib paths for distribution
        working-directory: llama-cpp-build/build/bin
        run: |
          echo "Fixing dylib install names for portable distribution..."

          # List of dylibs to fix
          DYLIBS=(
            "libggml.dylib"
            "libggml.0.dylib"
            "libggml-base.dylib"
            "libggml-base.0.dylib"
            "libggml-cpu.dylib"
            "libggml-cpu.0.dylib"
            "libggml-metal.dylib"
            "libggml-metal.0.dylib"
            "libggml-blas.dylib"
            "libggml-blas.0.dylib"
            "libllama.dylib"
            "libllama.0.dylib"
            "libmtmd.dylib"
            "libmtmd.0.dylib"
          )

          # Fix the install name of each dylib to use @loader_path
          for dylib in "${DYLIBS[@]}"; do
            if [ -f "$dylib" ]; then
              echo "Fixing install name for $dylib"
              install_name_tool -id "@loader_path/$dylib" "$dylib" 2>/dev/null || true
            fi
          done

          # Fix all executables and dylibs to reference dylibs via @loader_path
          for file in *; do
            if [ -f "$file" ] && (file "$file" | grep -q "Mach-O"); then
              echo "Fixing references in $file"
              for dylib in "${DYLIBS[@]}"; do
                # Get the current path recorded in the binary
                current_path=$(otool -L "$file" 2>/dev/null | grep "$dylib" | awk '{print $1}' | head -1)
                if [ -n "$current_path" ] && [ "$current_path" != "@loader_path/$dylib" ]; then
                  echo "  Changing $current_path -> @loader_path/$dylib"
                  install_name_tool -change "$current_path" "@loader_path/$dylib" "$file" 2>/dev/null || true
                fi
              done
            fi
          done

          # Verify the fix
          echo ""
          echo "Verifying llama-cli dependencies:"
          otool -L llama-cli | head -20

      - name: Organize binaries for packaging
        run: |
          # Create a flat directory for all files
          mkdir -p "llama-output"

          # Copy ALL files from build/bin directory (binaries, Metal shaders, etc.)
          echo "Copying all files from build/bin..."
          if [ -d "llama-cpp-build/build/bin" ]; then
            cp -r llama-cpp-build/build/bin/* "llama-output/" 2>/dev/null || true
            echo "Copied all build/bin contents"
          fi

          # Copy Metal headers
          echo "Copying Metal headers..."
          find llama-cpp-build -name "ggml-metal.h" -exec cp {} "llama-output/" \; 2>/dev/null || true
          find llama-cpp-build -name "ggml-metal-impl.h" -exec cp {} "llama-output/" \; 2>/dev/null || true
          find llama-cpp-build -name "ggml-common.h" -exec cp {} "llama-output/" \; 2>/dev/null || true

          # Copy all versioned dylibs (like libggml-base.0.dylib)
          echo "Copying dynamic libraries..."
          find llama-cpp-build/build -name "*.dylib" -exec cp {} "llama-output/" \; 2>/dev/null || true

          # Copy Metal shader file if not already copied
          find llama-cpp-build -name "ggml-metal.metal" -exec cp {} "llama-output/" \; 2>/dev/null || true

          # List compiled files
          echo "Compiled llama.cpp files:"
          ls -lh "llama-output/"

      - name: Create ZIP archive
        run: |
          cd "llama-output" && zip -r "../llama-macos-metal.zip" . && cd ..

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: llama-macos-metal-${{ github.event.inputs.release_tag || steps.llama_tag.outputs.tag }}
          path: llama-macos-metal.zip
          retention-days: ${{ inputs.retention_days }}

  build-windows-cuda:
    name: Windows (CUDA)
    runs-on: windows-latest

    steps:
      - name: Checkout project repository
        uses: actions/checkout@v4

      - name: Read llama.cpp tag from package.json
        id: llama_tag
        shell: bash
        run: |
          LLAMA_TAG=$(grep '"llamacpp_tag"' package.json | cut -d'"' -f4)
          echo "tag=$LLAMA_TAG" >> $GITHUB_OUTPUT
          echo "Building llama.cpp at tag: $LLAMA_TAG"

      - name: Clone llama.cpp repository
        uses: actions/checkout@v4
        with:
          repository: ggml-org/llama.cpp
          ref: ${{ github.event.inputs.release_tag || steps.llama_tag.outputs.tag }}
          submodules: recursive
          path: llama-cpp-build

      - name: Setup MSVC
        uses: microsoft/setup-msbuild@v2

      - name: Install CUDA Toolkit
        uses: Jimver/cuda-toolkit@v0.2.16
        with:
          cuda: '12.4.0'
          method: 'network'

      - name: Verify CUDA installation
        run: |
          nvcc --version
          echo "CUDA_PATH: $env:CUDA_PATH"

      - name: Configure CMake with CUDA
        working-directory: llama-cpp-build
        run: |
          cmake -B build `
            -G "Visual Studio 17 2022" `
            -A x64 `
            -DCMAKE_BUILD_TYPE=Release `
            -DGGML_CUDA=ON `
            -DLLAMA_CURL=OFF `
            -DLLAMA_BUILD_TESTS=OFF `
            -DLLAMA_BUILD_EXAMPLES=ON `
            -DCMAKE_CUDA_ARCHITECTURES="60;61;70;75;80;86;89;90"

      - name: Build
        working-directory: llama-cpp-build
        run: |
          cmake --build build --config Release --parallel

      - name: Organize binaries for packaging
        shell: powershell
        run: |
          # Create a flat directory for all files
          New-Item -ItemType Directory -Force -Path "llama-output"

          # Debug: Check what directories exist
          Write-Host "[DEBUG] Checking build directory structure..."
          if (Test-Path "llama-cpp-build\build\bin") {
            Write-Host "[OK] Found llama-cpp-build\build\bin"
            $dirs = Get-ChildItem "llama-cpp-build\build\bin" -Directory
            foreach ($dir in $dirs) {
              Write-Host "  [DIR] $($dir.Name)"
            }
          }
          else {
            Write-Host "[ERROR] llama-cpp-build\build\bin does not exist"
          }

          # Copy ALL files from build/bin/Release directory (executables, DLLs, etc.)
          Write-Host "[INFO] Copying all files from build/bin/Release..."
          if (Test-Path "llama-cpp-build\build\bin\Release") {
            $files = Get-ChildItem "llama-cpp-build\build\bin\Release" -File
            if ($files.Count -eq 0) {
              Write-Host "[WARNING] No files found in llama-cpp-build\build\bin\Release"
            }
            else {
              foreach ($file in $files) {
                Copy-Item $file.FullName "llama-output" -ErrorAction Stop
                Write-Host "  [OK] Copied: $($file.Name)"
              }
            }
          }
          else {
            Write-Host "[ERROR] Directory llama-cpp-build\build\bin\Release does not exist!"
            exit 1
          }

          # Copy CUDA runtime DLLs
          Write-Host "[INFO] Copying CUDA runtime DLLs..."
          $cudaPath = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4\bin"
          if (Test-Path $cudaPath) {
            Copy-Item "$cudaPath\cublas64_12.dll" "llama-output" -ErrorAction Continue
            Copy-Item "$cudaPath\cublasLt64_12.dll" "llama-output" -ErrorAction Continue
            Copy-Item "$cudaPath\cudart64_12.dll" "llama-output" -ErrorAction Continue
            Write-Host "  [OK] Copied CUDA runtime DLLs"
          }
          else {
            Write-Host "[WARNING] CUDA path not found at $cudaPath"
          }

          # List compiled files
          Write-Host "[INFO] Compiled llama.cpp files:"
          $outputFiles = Get-ChildItem "llama-output"
          if ($outputFiles.Count -eq 0) {
            Write-Host "[ERROR] No files in llama-output directory!"
            exit 1
          }
          $outputFiles | Format-Table Name, Length

      - name: Create ZIP archive
        shell: powershell
        run: |
          # Verify llama-output has files before compressing
          $fileCount = (Get-ChildItem "llama-output" -File).Count
          Write-Host "[INFO] Files to compress: $fileCount"

          if ($fileCount -eq 0) {
            Write-Host "[ERROR] llama-output is empty, cannot create archive"
            exit 1
          }

          Compress-Archive -Path "llama-output\*" -DestinationPath "llama-windows-cuda.zip" -ErrorAction Stop

          # Verify zip was created
          if (Test-Path "llama-windows-cuda.zip") {
            $zipSize = (Get-Item "llama-windows-cuda.zip").Length / 1MB
            Write-Host "[OK] Created llama-windows-cuda.zip ($([math]::Round($zipSize, 2)) MB)"
          } else {
            Write-Host "[ERROR] Failed to create llama-windows-cuda.zip"
            exit 1
          }

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: llama-windows-cuda-${{ github.event.inputs.release_tag || steps.llama_tag.outputs.tag }}
          path: llama-windows-cuda.zip
          retention-days: ${{ inputs.retention_days }}

  # CPU-only build for Windows
  build-windows-cpu:
    name: Windows (CPU)
    runs-on: windows-latest

    steps:
      - name: Checkout project repository
        uses: actions/checkout@v4

      - name: Read llama.cpp tag from package.json
        id: llama_tag
        shell: bash
        run: |
          LLAMA_TAG=$(grep '"llamacpp_tag"' package.json | cut -d'"' -f4)
          echo "tag=$LLAMA_TAG" >> $GITHUB_OUTPUT
          echo "Building llama.cpp at tag: $LLAMA_TAG"

      - name: Clone llama.cpp repository
        uses: actions/checkout@v4
        with:
          repository: ggml-org/llama.cpp
          ref: ${{ github.event.inputs.release_tag || steps.llama_tag.outputs.tag }}
          submodules: recursive
          path: llama-cpp-build

      - name: Setup MSVC
        uses: microsoft/setup-msbuild@v2

      - name: Configure CMake (CPU optimized)
        working-directory: llama-cpp-build
        run: |
          cmake -B build `
            -G "Visual Studio 17 2022" `
            -A x64 `
            -DCMAKE_BUILD_TYPE=Release `
            -DLLAMA_CURL=OFF `
            -DLLAMA_BUILD_TESTS=OFF `
            -DLLAMA_BUILD_EXAMPLES=ON `
            -DGGML_AVX2=ON `
            -DGGML_AVX=ON `
            -DGGML_F16C=ON `
            -DGGML_FMA=ON

      - name: Build
        working-directory: llama-cpp-build
        run: |
          cmake --build build --config Release --parallel

      - name: Organize binaries for packaging
        shell: powershell
        run: |
          # Create a flat directory for all files
          New-Item -ItemType Directory -Force -Path "llama-output"

          # Debug: Check what directories exist
          Write-Host "[DEBUG] Checking build directory structure..."
          if (Test-Path "llama-cpp-build\build\bin") {
            Write-Host "[OK] Found llama-cpp-build\build\bin"
            $dirs = Get-ChildItem "llama-cpp-build\build\bin" -Directory
            foreach ($dir in $dirs) {
              Write-Host "  [DIR] $($dir.Name)"
            }
          }
          else {
            Write-Host "[ERROR] llama-cpp-build\build\bin does not exist"
          }

          # Copy ALL files from build/bin/Release directory (executables, DLLs, etc.)
          Write-Host "[INFO] Copying all files from build/bin/Release..."
          if (Test-Path "llama-cpp-build\build\bin\Release") {
            $files = Get-ChildItem "llama-cpp-build\build\bin\Release" -File
            if ($files.Count -eq 0) {
              Write-Host "[WARNING] No files found in llama-cpp-build\build\bin\Release"
            }
            else {
              foreach ($file in $files) {
                Copy-Item $file.FullName "llama-output" -ErrorAction Stop
                Write-Host "  [OK] Copied: $($file.Name)"
              }
            }
          }
          else {
            Write-Host "[ERROR] Directory llama-cpp-build\build\bin\Release does not exist!"
            exit 1
          }

          # List compiled files
          Write-Host "[INFO] Compiled llama.cpp files:"
          $outputFiles = Get-ChildItem "llama-output"
          if ($outputFiles.Count -eq 0) {
            Write-Host "[ERROR] No files in llama-output directory!"
            exit 1
          }
          $outputFiles | Format-Table Name, Length

      - name: Create ZIP archive
        shell: powershell
        run: |
          # Verify llama-output has files before compressing
          $fileCount = (Get-ChildItem "llama-output" -File).Count
          Write-Host "[INFO] Files to compress: $fileCount"

          if ($fileCount -eq 0) {
            Write-Host "[ERROR] llama-output is empty, cannot create archive"
            exit 1
          }

          Compress-Archive -Path "llama-output\*" -DestinationPath "llama-windows-cpu.zip" -ErrorAction Stop

          # Verify zip was created
          if (Test-Path "llama-windows-cpu.zip") {
            $zipSize = (Get-Item "llama-windows-cpu.zip").Length / 1MB
            Write-Host "[OK] Created llama-windows-cpu.zip ($([math]::Round($zipSize, 2)) MB)"
          } else {
            Write-Host "[ERROR] Failed to create llama-windows-cpu.zip"
            exit 1
          }

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: llama-windows-cpu-${{ github.event.inputs.release_tag || steps.llama_tag.outputs.tag }}
          path: llama-windows-cpu.zip
          retention-days: ${{ inputs.retention_days }}

  create-release:
    name: Create Release Package
    needs: [build-macos-metal, build-windows-cuda, build-windows-cpu]
    runs-on: ubuntu-latest

    steps:
      - name: Checkout project repository
        uses: actions/checkout@v4

      - name: Read llama.cpp tag from package.json
        id: llama_tag
        run: |
          LLAMA_TAG=$(grep '"llamacpp_tag"' package.json | cut -d'"' -f4)
          echo "tag=$LLAMA_TAG" >> $GITHUB_OUTPUT
          echo "Using llama.cpp tag: $LLAMA_TAG"

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Display artifacts
        run: |
          echo "Build Artifacts:"
          find artifacts -name "*.zip" -exec ls -lh {} \;

      - name: Create build summary
        run: |
          cat > build-summary.md << EOF
          # llama.cpp Build Summary

          ## GPU-Accelerated Builds Available

          ### macOS with Metal (Universal Binary)
          - **Universal Binary**: Runs natively on both Apple Silicon and Intel Macs
          - Full Metal GPU acceleration for all Macs with Metal support
          - Accelerate framework optimizations
          - Embedded Metal shaders for easy distribution
          - Architecture: arm64 + x86_64 (universal)

          ### Windows with CUDA
          - **NVIDIA GPUs**: Full CUDA acceleration
          - Supports RTX 20/30/40 series and newer
          - Includes cuBLAS optimizations
          - Architecture support: SM 60, 61, 70, 75, 80, 86, 89, 90

          ### Windows CPU-Only (Fallback)
          - Optimized for AVX/AVX2 processors
          - For systems without NVIDIA GPUs
          - Multi-threaded performance

          ## Performance Expectations

          | Configuration | 7B Model (Q4) | 13B Model (Q4) |
          |--------------|---------------|----------------|
          | Apple M2 Max | ~100 tok/s | ~60 tok/s |
          | RTX 4090 | ~150 tok/s | ~90 tok/s |
          | RTX 3080 | ~80 tok/s | ~45 tok/s |
          | CPU (8-core) | ~10 tok/s | ~5 tok/s |

          *Actual performance varies by model, quantization, and context size

          ## Which Build to Choose?

          - **macOS users**: Use the universal Metal build (works on all Macs)
          - **Windows + NVIDIA GPU**: Use the CUDA build
          - **Windows + AMD/Intel GPU**: Use the CPU build
          - **No GPU**: Use the CPU build

          ## Installation

          1. Download the appropriate ZIP file
          2. Extract out all the binaries, drivers, dlls, etc
          3. Copy the contents to project \`servers/llama.cpp/\`
          4. The binaries are now ready to use with your app or bundle!

          ## Resources

          - [llama.cpp GitHub](https://github.com/ggml-org/llama.cpp)
          - [GGUF Models on Hugging Face](https://huggingface.co/models?library=gguf&sort=trending)
          - [Model Quantization Guide](https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md)

          ---
          Build Date: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          Llama release tag: ${{ github.event.inputs.release_tag || steps.llama_tag.outputs.tag }}
          EOF

          cat build-summary.md

      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: build-summary-${{ github.event.inputs.release_tag || steps.llama_tag.outputs.tag }}
          path: build-summary.md
          retention-days: ${{ inputs.retention_days }}
