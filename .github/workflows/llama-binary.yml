name: Build llama.cpp (GPU)

on:
  workflow_dispatch:
    inputs:
      release_tag:
        description: 'Specific release tag or commit to build (leave empty to use package.json)'
        required: false
        type: string
  push:
    branches:
      - feature/HBAI-413
    paths:
      - '.github/workflows/llama-binary.yml'

jobs:
  build-macos-metal:
    name: macOS (Universal - Metal + Accelerate)
    runs-on: macos-latest

    steps:
      - name: Checkout project repository
        uses: actions/checkout@v4

      - name: Read llama.cpp tag from package.json
        id: llama_tag
        run: |
          LLAMA_TAG=$(grep '"llamacpp_tag"' package.json | cut -d'"' -f4)
          echo "tag=$LLAMA_TAG" >> $GITHUB_OUTPUT
          echo "Building llama.cpp at tag: $LLAMA_TAG"

      - name: Clone llama.cpp repository
        uses: actions/checkout@v4
        with:
          repository: ggml-org/llama.cpp
          ref: ${{ github.event.inputs.release_tag || steps.llama_tag.outputs.tag }}
          submodules: recursive
          path: llama-cpp-build

      - name: Install dependencies
        run: |
          brew install cmake
          # Verify Xcode command line tools for Metal compiler
          xcode-select --print-path

      - name: Configure with Metal and Accelerate (Universal Binary)
        working-directory: llama-cpp-build
        run: |
          cmake -B build \
            -DCMAKE_BUILD_TYPE=Release \
            -DCMAKE_OSX_ARCHITECTURES="arm64;x86_64" \
            -DGGML_METAL=ON \
            -DLLAMA_CURL=OFF \
            -DLLAMA_BUILD_TESTS=OFF \
            -DLLAMA_BUILD_EXAMPLES=ON \
            -DLLAMA_NATIVE=OFF

      - name: Build
        working-directory: llama-cpp-build
        run: |
          cmake --build build --config Release -j$(sysctl -n hw.ncpu)

      - name: Test Metal support
        working-directory: llama-cpp-build
        run: |
          # Verify binaries are built
          if [ -f "build/bin/llama-cli" ]; then
            file build/bin/llama-cli
            echo "âœ… Binary built successfully"
          fi

      - name: Organize binaries into expected structure
        run: |
          # Create the directory structure that matches your app's expectations
          mkdir -p "_deps/servers/llama.cpp"

          # Copy ALL files from build/bin directory (binaries, Metal shaders, etc.)
          echo "ğŸ“‹ Copying all files from build/bin..."
          if [ -d "llama-cpp-build/build/bin" ]; then
            cp -r llama-cpp-build/build/bin/* "_deps/servers/llama.cpp/" 2>/dev/null || true
            echo "âœ… Copied all build/bin contents"
          fi

          # Copy Metal headers
          echo "ğŸ“‹ Copying Metal headers..."
          find llama-cpp-build -name "ggml-metal.h" -exec cp {} "_deps/servers/llama.cpp/" \; 2>/dev/null || true
          find llama-cpp-build -name "ggml-metal-impl.h" -exec cp {} "_deps/servers/llama.cpp/" \; 2>/dev/null || true
          find llama-cpp-build -name "ggml-common.h" -exec cp {} "_deps/servers/llama.cpp/" \; 2>/dev/null || true

          # Copy all versioned dylibs (like libggml-base.0.dylib)
          echo "ğŸ“‹ Copying dynamic libraries..."
          find llama-cpp-build/build -name "*.dylib" -exec cp {} "_deps/servers/llama.cpp/" \; 2>/dev/null || true

          # Copy Metal shader file if not already copied
          find llama-cpp-build -name "ggml-metal.metal" -exec cp {} "_deps/servers/llama.cpp/" \; 2>/dev/null || true

          # List compiled files
          echo "ğŸ“¦ Compiled llama.cpp files:"
          ls -lh "_deps/servers/llama.cpp/"

      - name: Create ZIP archive
        run: |
          zip -r "llama-macos-universal-metal.zip" "_deps/"

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: llama-macos-universal-metal
          path: llama-macos-universal-metal.zip
          retention-days: 7

  build-windows-cuda:
    name: Windows (CUDA)
    runs-on: windows-latest

    steps:
      - name: Checkout project repository
        uses: actions/checkout@v4

      - name: Read llama.cpp tag from package.json
        id: llama_tag
        shell: bash
        run: |
          LLAMA_TAG=$(grep '"llamacpp_tag"' package.json | cut -d'"' -f4)
          echo "tag=$LLAMA_TAG" >> $GITHUB_OUTPUT
          echo "Building llama.cpp at tag: $LLAMA_TAG"

      - name: Clone llama.cpp repository
        uses: actions/checkout@v4
        with:
          repository: ggml-org/llama.cpp
          ref: ${{ github.event.inputs.release_tag || steps.llama_tag.outputs.tag }}
          submodules: recursive
          path: llama-cpp-build

      - name: Setup MSVC
        uses: microsoft/setup-msbuild@v2

      - name: Install CUDA Toolkit
        uses: Jimver/cuda-toolkit@v0.2.16
        with:
          cuda: '12.4.0'
          method: 'network'

      - name: Verify CUDA installation
        run: |
          nvcc --version
          echo "CUDA_PATH: $env:CUDA_PATH"

      - name: Configure CMake with CUDA
        working-directory: llama-cpp-build
        run: |
          cmake -B build `
            -G "Visual Studio 17 2022" `
            -A x64 `
            -DCMAKE_BUILD_TYPE=Release `
            -DGGML_CUDA=ON `
            -DLLAMA_CURL=OFF `
            -DLLAMA_BUILD_TESTS=OFF `
            -DLLAMA_BUILD_EXAMPLES=ON `
            -DCMAKE_CUDA_ARCHITECTURES="60;61;70;75;80;86;89;90"

      - name: Build
        working-directory: llama-cpp-build
        run: |
          cmake --build build --config Release --parallel

      - name: Organize binaries into expected structure
        shell: powershell
        run: |
          # Create the directory structure that matches your app's expectations
          New-Item -ItemType Directory -Force -Path "_deps\servers\llama.cpp"

          # Copy ALL files from build/bin/Release directory (executables, DLLs, etc.)
          Write-Host "ğŸ“‹ Copying all files from build/bin/Release..."
          if (Test-Path "llama-cpp-build\build\bin\Release") {
            Get-ChildItem "llama-cpp-build\build\bin\Release" -File | ForEach-Object {
              Copy-Item $_.FullName "_deps\servers\llama.cpp" -ErrorAction SilentlyContinue
              Write-Host "  âœ… Copied: $($_.Name)"
            }
          }

          # Copy CUDA runtime DLLs
          Write-Host "ğŸ“‹ Copying CUDA runtime DLLs..."
          $cudaPath = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4\bin"
          if (Test-Path $cudaPath) {
            Copy-Item "$cudaPath\cublas64_12.dll" "_deps\servers\llama.cpp" -ErrorAction SilentlyContinue
            Copy-Item "$cudaPath\cublasLt64_12.dll" "_deps\servers\llama.cpp" -ErrorAction SilentlyContinue
            Copy-Item "$cudaPath\cudart64_12.dll" "_deps\servers\llama.cpp" -ErrorAction SilentlyContinue
            Write-Host "  âœ… Copied CUDA runtime DLLs"
          }

          # List compiled files
          Write-Host "ğŸ“¦ Compiled llama.cpp files:"
          Get-ChildItem "_deps\servers\llama.cpp" | Format-Table Name, Length

      - name: Create ZIP archive
        shell: powershell
        run: |
          Compress-Archive -Path "_deps\*" -DestinationPath "llama-windows-cuda.zip"

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: llama-windows-cuda
          path: llama-windows-cuda.zip
          retention-days: 7

  # Optional: CPU-only fallback build for Windows
  build-windows-cpu:
    name: Windows (CPU-only fallback)
    runs-on: windows-latest

    steps:
      - name: Checkout project repository
        uses: actions/checkout@v4

      - name: Read llama.cpp tag from package.json
        id: llama_tag
        shell: bash
        run: |
          LLAMA_TAG=$(grep '"llamacpp_tag"' package.json | cut -d'"' -f4)
          echo "tag=$LLAMA_TAG" >> $GITHUB_OUTPUT
          echo "Building llama.cpp at tag: $LLAMA_TAG"

      - name: Clone llama.cpp repository
        uses: actions/checkout@v4
        with:
          repository: ggml-org/llama.cpp
          ref: ${{ github.event.inputs.release_tag || steps.llama_tag.outputs.tag }}
          submodules: recursive
          path: llama-cpp-build

      - name: Setup MSVC
        uses: microsoft/setup-msbuild@v2

      - name: Configure CMake (CPU optimized)
        working-directory: llama-cpp-build
        run: |
          cmake -B build `
            -G "Visual Studio 17 2022" `
            -A x64 `
            -DCMAKE_BUILD_TYPE=Release `
            -DLLAMA_CURL=OFF `
            -DLLAMA_BUILD_TESTS=OFF `
            -DLLAMA_BUILD_EXAMPLES=ON `
            -DGGML_AVX2=ON `
            -DGGML_AVX=ON `
            -DGGML_F16C=ON `
            -DGGML_FMA=ON

      - name: Build
        working-directory: llama-cpp-build
        run: |
          cmake --build build --config Release --parallel

      - name: Organize binaries into expected structure
        shell: powershell
        run: |
          # Create the directory structure that matches your app's expectations
          New-Item -ItemType Directory -Force -Path "_deps\servers\llama.cpp"

          # Copy ALL files from build/bin/Release directory (executables, DLLs, etc.)
          Write-Host "ğŸ“‹ Copying all files from build/bin/Release..."
          if (Test-Path "llama-cpp-build\build\bin\Release") {
            Get-ChildItem "llama-cpp-build\build\bin\Release" -File | ForEach-Object {
              Copy-Item $_.FullName "_deps\servers\llama.cpp" -ErrorAction SilentlyContinue
              Write-Host "  âœ… Copied: $($_.Name)"
            }
          }

          # List compiled files
          Write-Host "ğŸ“¦ Compiled llama.cpp files:"
          Get-ChildItem "_deps\servers\llama.cpp" | Format-Table Name, Length

      - name: Create ZIP archive
        shell: powershell
        run: |
          Compress-Archive -Path "_deps\*" -DestinationPath "llama-windows-cpu.zip"

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: llama-windows-cpu
          path: llama-windows-cpu.zip
          retention-days: 7

  create-release:
    name: Create Release Package
    needs: [build-macos-metal, build-windows-cuda, build-windows-cpu]
    runs-on: ubuntu-latest

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Display artifacts
        run: |
          echo "ğŸ“¦ Build Artifacts:"
          find artifacts -name "*.zip" -exec ls -lh {} \;

      - name: Create build summary
        run: |
          cat > build-summary.md << 'EOF'
          # llama.cpp Build Summary

          ## ğŸš€ GPU-Accelerated Builds Available

          ### macOS with Metal (Universal Binary)
          - **Universal Binary**: Runs natively on both Apple Silicon and Intel Macs
          - âœ… Full Metal GPU acceleration for all Macs with Metal support
          - âœ… Accelerate framework optimizations
          - âœ… Embedded Metal shaders for easy distribution
          - ğŸ—ï¸ Architecture: arm64 + x86_64 (universal)

          ### Windows with CUDA
          - **NVIDIA GPUs**: Full CUDA acceleration
          - Supports RTX 20/30/40 series and newer
          - Includes cuBLAS optimizations
          - Architecture support: SM 60, 61, 70, 75, 80, 86, 89, 90

          ### Windows CPU-Only (Fallback)
          - Optimized for AVX/AVX2 processors
          - For systems without NVIDIA GPUs
          - Multi-threaded performance

          ## ğŸ“Š Performance Expectations

          | Configuration | 7B Model (Q4) | 13B Model (Q4) |
          |--------------|---------------|----------------|
          | Apple M2 Max | ~100 tok/s | ~60 tok/s |
          | RTX 4090 | ~150 tok/s | ~90 tok/s |
          | RTX 3080 | ~80 tok/s | ~45 tok/s |
          | CPU (8-core) | ~10 tok/s | ~5 tok/s |

          *Actual performance varies by model, quantization, and context size

          ## ğŸ¯ Which Build to Choose?

          - **macOS users**: Use the universal Metal build (works on all Macs)
          - **Windows + NVIDIA GPU**: Use the CUDA build
          - **Windows + AMD/Intel GPU**: Use the CPU build
          - **No GPU**: Use the CPU build

          ## ğŸ“¥ Installation

          1. Download the appropriate ZIP file
          2. Extract to reveal the `_deps/servers/llama.cpp/` directory structure
          3. Copy the `_deps` folder to your project root or commit to your repository
          4. The binaries are now ready to use with your app!

          ## ğŸ”— Resources

          - [llama.cpp GitHub](https://github.com/ggml-org/llama.cpp)
          - [GGUF Models on Hugging Face](https://huggingface.co/models?library=gguf&sort=trending)
          - [Model Quantization Guide](https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md)

          ---
          Build Date: $(date)
          EOF

          cat build-summary.md

      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: build-summary
          path: build-summary.md
          retention-days: 30
